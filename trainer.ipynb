{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "xMX8NrPQppvi",
        "outputId": "e65d608e-093e-417c-d6ff-a180c07bb21d"
      },
      "outputs": [],
      "source": [
        "############ TRAINING THE MODEL #################\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import T5Model, T5Tokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import default_data_collator\n",
        "from transformers import TrainingArguments, Traine\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, auc\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import bz2\n",
        "import mgzip\n",
        "import gc\n",
        "from functools import reduce\n",
        "import bisect\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm\n",
        "import gc\n",
        "from datasets import load_metric\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# Specify file paths\n",
        "tokenised_data_path = \"/content/drive/My Drive/Colab Notebooks/Dissertation Code/AWS/Data/256\"\n",
        "\n",
        "\n",
        "\n",
        "########## PREPARE DATASETS ##########\n",
        "# Importing training, test and validation x sets\n",
        "print(\"\\nLoading training dataset\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_train.gz\", \"rb\") as f:\n",
        "    x_train = pickle.load(f)\n",
        "print(\"\\nLoading validation dataset\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_val.gz\", \"rb\") as f:\n",
        "    x_val = pickle.load(f)\n",
        "\n",
        "\n",
        "# Import y sets\n",
        "print(\"\\nLoading training y labels\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_y_train.gz\", \"rb\") as f:\n",
        "    y_train = pickle.load(f)\n",
        "print(\"\\nLoading validation y labels\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_y_val.gz\", \"rb\") as f:\n",
        "    y_val = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "########## LOAD MODEL ##########\n",
        "print(\"\\nLoading model\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"Rostlab/prot_bert_bfd\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Define function to extract attnetion mask and sequences from encoded files and combine with labels into dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, encoding, labels):\n",
        "        self.encoding = encoding\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    \n",
        "\n",
        "\n",
        "########## TRAIN MODEL ##########\n",
        "# Provide model arguments\n",
        "print(\"\\nPreparing model\")\n",
        "torch.cuda.empty_cache()\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=24,\n",
        "    per_device_eval_batch_size=24,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=30,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    gradient_accumulation_steps=12,            # accumulate gradients from smaller batches before updating model weights: speeds up training, reduces RAM usage\n",
        "    fp16=True,                                 # mixed precision training: use mixture of float32 and float16 precision for faster training - also needs GPU\n",
        "    load_best_model_at_end=True,\n",
        "    prediction_loss_only=False                 # set true to not calculate metrics and save RAM\n",
        ")\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()          # may be redundant\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "def calculate_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    print(f\"labels: {labels}\")\n",
        "    print(f\"predictions: {predictions}\")\n",
        "    return {\n",
        "        \"f1_score\": f1_score(labels, predictions, average='micro'),\n",
        "        \"precision\": precision_score(labels, predictions, average='micro'),\n",
        "        \"recall\": recall_score(labels, predictions, average='micro'),\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"eval_f1_score\": f1_score(labels, predictions, average='micro')\n",
        "    }\n",
        "    \n",
        "\n",
        "x_train = {'input_ids': x_train.input_ids, 'attention_mask': x_train.attention_mask}\n",
        "x_val =  {'input_ids': x_val.input_ids, 'attention_mask': x_val.attention_mask}\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(y_train))\n",
        "print(len(x_val))\n",
        "print(len(y_val))\n",
        "\n",
        "\n",
        "# Turn input sequences and labels into datasets\n",
        "train_dataset = MyDataset(x_train, y_train)\n",
        "val_dataset = MyDataset(x_val, y_val)\n",
        "\n",
        "\n",
        "# Verify number of sequences and class proportions\n",
        "class_counts = {}\n",
        "for i in range(len(train_dataset)):\n",
        "    label = train_dataset[i]['labels'].item()\n",
        "    if label in class_counts:\n",
        "        class_counts[label] += 1\n",
        "    else:\n",
        "        class_counts[label] = 1\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create custom trainer class to calculate metrics\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.custom_metrics_history = []\n",
        "\n",
        "    def on_evaluate_end(self):\n",
        "        self.custom_metrics_history.append(self.state.metrics.copy())\n",
        "\n",
        "# Initialise custom trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=calculate_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "# Clean up RAM and train the model\n",
        "torch.cuda.empty_cache()\n",
        "del x_val, y_val\n",
        "gc.collect()\n",
        "\n",
        "trainer.train()\n",
        "train_history = trainer.state.log_history\n",
        "\n",
        "\n",
        "# Save the trained model\n",
        "model_location = \"results\"                      # results directory will be created by trainer\n",
        "model.save_pretrained(model_location)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################################################################################################################################################\n",
        "# The code below was an attempt to calculate metrics (f1 score, precision, recall, accuracy) during traing and plot the results\n",
        "# Unfortunately, despite extensive efforts the metrics produced still seem to be inaccurate and the code fails to produce any graphs\n",
        "# This code does not affect the trianing of the model, however, and is preserved for potential further development\n",
        "\n",
        "\n",
        "\n",
        "## Get the model's predictions on the training set\n",
        "train_predictions, train_labels, _ = trainer.predict(x_train)\n",
        "train_prob_estimates = torch.softmax(torch.tensor(train_predictions).float(), dim=-1)[:, 1].numpy()\n",
        "\n",
        "\n",
        "train_precision, train_recall, _ = precision_recall_curve(y_train, train_prob_estimates)\n",
        "train_auc_score = auc(train_recall, train_precision)\n",
        "\n",
        "train_f1_score = f1_score(y_train, np.argmax(train_predictions, axis=-1))\n",
        "train_accuracy = accuracy_score(y_train, np.argmax(train_predictions, axis=-1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f1_scores = [epoch_dict['f1_score'] for epoch_dict in trainer.custom_metrics_history]\n",
        "eval_f1_scores = [epoch_dict['eval_f1_score'] for epoch_dict in trainer.custom_metrics_history]\n",
        "precision = [epoch_dict['precision'] for epoch_dict in trainer.custom_metrics_history]\n",
        "eval_precision = [epoch_dict['eval_precision'] for epoch_dict in trainer.custom_metrics_history]\n",
        "recall = [epoch_dict['recall'] for epoch_dict in trainer.custom_metrics_history]\n",
        "eval_recall = [epoch_dict['eval_recall'] for epoch_dict in trainer.custom_metrics_history]\n",
        "accuracy = [epoch_dict['accuracy'] for epoch_dict in trainer.custom_metrics_history]\n",
        "eval_accuracy = [epoch_dict['eval_accuracy'] for epoch_dict in trainer.custom_metrics_history]\n",
        "\n",
        "print(f\"f1_scores: {f1_scores}\")\n",
        "print(f\"eval_f1_scores: {eval_f1_scores}\")\n",
        "print(f\"accuracy: {accuracy}\")\n",
        "print(f\"eval_accuracy: {eval_accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "def plot_metrics(train_metric_values, val_metric_values, metric_name, x_label, y_label):\n",
        "    epochs = range(1, len(train_metric_values) + 1)\n",
        "    \n",
        "    plt.plot(epochs, train_metric_values, 'bo', label=f'Training {metric_name}')\n",
        "    plt.plot(epochs, val_metric_values, 'r', label=f'Validation {metric_name}')\n",
        "    plt.title(f'Training and Validation {metric_name}')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Use the modified function to plot the metrics\n",
        "plot_metrics(f1_scores, eval_f1_scores, 'F1 Score', 'Epochs', 'F1 Score')\n",
        "plot_metrics(precision, eval_precision, 'Precision', 'Epochs', 'Precision')\n",
        "plot_metrics(recall, eval_recall, 'Recall', 'Epochs', 'Recall')\n",
        "plot_metrics(accuracy, eval_accuracy, 'Accuracy', 'Epochs', 'Accuracy')\n",
        "\n",
        "\n",
        "def plot_metrics(train_metric_values, val_metric_values, metric_name, x_label, y_label):\n",
        "    epochs = range(1, len(train_metric_values) + 1)\n",
        "    \n",
        "    plt.plot(epochs, train_metric_values, 'bo', label=f'Training {metric_name}')\n",
        "    plt.plot(epochs, val_metric_values, 'r', label=f'Validation {metric_name}')\n",
        "    plt.title(f'Training and Validation {metric_name}')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# Probabiliity estimates for resisting heat (positive class)\n",
        "# eval_outputs = trainer.predict(val_dataset)\n",
        "eval_outputs = trainer.predict(val_dataset)\n",
        "logits = eval_outputs.predictions\n",
        "prob_estimates = np.exp(logits[:, 1]) / (np.exp(logits[:, 0]) + np.exp(logits[:, 1]))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_val, prob_estimates)\n",
        "auc_score = auc(recall, precision)\n",
        "\n",
        "plt.plot(recall, precision, 'b', label=f'AUC = {auc_score:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def plot_save_metrics(train_metric_values, val_metric_values, metric_name, x_label, y_label, output_file):\n",
        "    epochs = range(1, len(train_metric_values) + 1)\n",
        "    \n",
        "    plt.plot(epochs, train_metric_values, 'bo', label=f'Training {metric_name}')\n",
        "    plt.plot(epochs, val_metric_values, 'r', label=f'Validation {metric_name}')\n",
        "    plt.title(f'Training and Validation {metric_name}')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend()\n",
        "    plt.savefig(output_file)\n",
        "    plt.clf()\n",
        "\n",
        "def plot_save_precision_recall_curve(precision, recall, auc_score, output_file):\n",
        "    plt.plot(recall, precision, 'b', label=f'AUC = {auc_score:.2f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.savefig(output_file)\n",
        "    plt.clf()\n",
        "\n",
        "# Call the updated functions and provide the output file names\n",
        "plot_save_metrics(f1_scores, eval_f1_scores, 'F1 Score', 'Epochs', 'F1 Score', 'f1.png')\n",
        "plot_save_metrics(precision, eval_precision, 'Precision', 'Epochs', 'Precision', 'precision.png')\n",
        "plot_save_metrics(recall, eval_recall, 'Recall', 'Epochs', 'Recall', 'recall.png')\n",
        "plot_save_metrics(accuracy, eval_accuracy, 'Accuracy', 'Epochs', 'Accuracy', 'accuracy.png')\n",
        "\n",
        "plot_save_precision_recall_curve(precision, recall, auc_score, 'precision_recall_curve.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79O6D0_l6OHu",
        "outputId": "5ad9a3b2-bb4c-48fc-9fc1-783e2810137a"
      },
      "outputs": [],
      "source": [
        "############ CALCULATING METRICS FOR LABELELD DATASETS ##################\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from transformers import BertModel, BertTokenizer#, DistilProtBert\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import T5Model, T5Tokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import default_data_collator\n",
        "from transformers import TrainingArguments, Trainer#, default_device\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, auc\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import bz2\n",
        "import mgzip\n",
        "import gc\n",
        "from functools import reduce\n",
        "import bisect\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm\n",
        "import gc\n",
        "from datasets import load_metric\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "# Specify filepaths\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/Dissertation Code/AWS/results\"\n",
        "tokenised_data_path = \"/content/drive/My Drive/Colab Notebooks/Dissertation Code/AWS/Data/256\"\n",
        "dataset = \"val\" # train, val or test\n",
        "\n",
        "\n",
        "# Load trained model\n",
        "config = AutoConfig.from_pretrained(f'{model_path}/config.json')\n",
        "model = BertForSequenceClassification.from_pretrained(f'{model_path}/pytorch_model.bin', config=config)\n",
        "\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# Load data\n",
        "print(\"\\nLoading dataset\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_{dataset}.gz\", \"rb\") as f:\n",
        "    x = pickle.load(f)\n",
        "\n",
        "print(\"\\nLoading labels\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_y_{dataset}.gz\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "\n",
        "y = torch.tensor(y)\n",
        "\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "test_dataset = EvalDataset(x, y)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "\n",
        "class_counts = {}\n",
        "for i in range(len(test_dataset)):\n",
        "    label = test_dataset[i]['labels'].item()\n",
        "    if label in class_counts:\n",
        "        class_counts[label] += 1\n",
        "    else:\n",
        "        class_counts[label] = 1\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "model.eval()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(predictions)\n",
        "\n",
        "f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "precision = precision_score(true_labels, pred_labels, average='weighted')\n",
        "recall = recall_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQGB9hnFE-mM",
        "outputId": "c2b5f83a-afbe-426c-f0a9-2a0dd37f6086"
      },
      "outputs": [],
      "source": [
        "############ INFERENCE WITH UNLABELLED DATASET ##################\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from transformers import BertModel, BertTokenizer#, DistilProtBert\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import T5Model, T5Tokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import default_data_collator\n",
        "from transformers import TrainingArguments, Trainer#, default_device\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, auc\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import bz2\n",
        "import mgzip\n",
        "import gc\n",
        "from functools import reduce\n",
        "import bisect\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm\n",
        "import gc\n",
        "from datasets import load_metric\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "# Specify filepaths\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/Dissertation Code/AWS/results\"\n",
        "tokenised_data_path = \"/content/drive/My Drive/Colab Notebooks/Dissertation Code/AWS/Data/256\"\n",
        "dataset = \"val\" # cauris, train, val or test\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Import model\n",
        "config = AutoConfig.from_pretrained(f'{model_path}/config.json')\n",
        "model = BertForSequenceClassification.from_pretrained(f'{model_path}/pytorch_model.bin', config=config)\n",
        "model.to(device)\n",
        "\n",
        "# Load unlabelled dataset\n",
        "print(\"\\nLoading unlabelled dataset\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_{dataset}.gz\", \"rb\") as f:\n",
        "    unlabelled_data = pickle.load(f)\n",
        "\n",
        "# Convert input_ids and attention_mask lists to tensors\n",
        "input_ids = torch.tensor(unlabelled_data['input_ids'])\n",
        "attention_mask = torch.tensor(unlabelled_data['attention_mask'])\n",
        "\n",
        "# Create DataLoader for the unlabeled dataset\n",
        "unlabeled_dataset = TensorDataset(input_ids, attention_mask)\n",
        "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=16)\n",
        "\n",
        "\n",
        "print(len(unlabelled_data))\n",
        "print(len(input_ids))\n",
        "\n",
        "\n",
        "# Perform inference on the unlabeled dataset\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(unlabeled_dataloader, desc=\"Inference\"):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits1\n",
        "        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Convert predictions to a pandas DataFrame\n",
        "predictions_df = pd.DataFrame(predictions, columns=['prediction'])\n",
        "\n",
        "# Save predictions as a CSV file\n",
        "predictions_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "\n",
        "# Save real labels as a CSV file\n",
        "print(\"\\nLoading evaluation labels\")\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_y_{dataset}.gz\", \"rb\") as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "y.to_csv(f'{dataset}_groundtruth.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaxbKe3yGz_y",
        "outputId": "66714dce-1f36-456f-9081-71eb81183232"
      },
      "outputs": [],
      "source": [
        "################ Baseline Metric Calculations ###############\n",
        "import csv\n",
        "import random\n",
        "import mgzip\n",
        "import pickle\n",
        "\n",
        "# Specify filepaths\n",
        "predictions_path = \"/content/drive/MyDrive/Colab Notebooks/Dissertation Code/AWS/results\"\n",
        "tokenised_data_path = \"/content/drive/My Drive/Colab Notebooks/Dissertation Code/AWS/Data/256\"\n",
        "dataset = \"test\"    # can also use train or val to see baselines for these\n",
        "\n",
        "def read_csv(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        labels = csv.reader(file)\n",
        "        next(labels)  # Skip the header row\n",
        "        return [int(row[0]) for row in labels]\n",
        "\n",
        "\n",
        "def calculate_confusion_matrix(predictions, ground_truth):\n",
        "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truth):\n",
        "        if pred == gt:\n",
        "            if pred == 1:\n",
        "                true_pos += 1\n",
        "            else:\n",
        "                true_neg += 1\n",
        "        else:\n",
        "            if pred == 1:\n",
        "                false_pos += 1\n",
        "            else:\n",
        "                false_neg += 1\n",
        "\n",
        "    return true_pos, true_neg, false_pos, false_neg\n",
        "\n",
        "\n",
        "def calculate_metrics(predictions, ground_truth):\n",
        "    true_pos, true_neg, false_pos, false_neg = calculate_confusion_matrix(predictions, ground_truth)\n",
        "\n",
        "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
        "    precision = true_pos / (true_pos + false_pos) if true_pos + false_pos > 0 else 0\n",
        "    recall = true_pos / (true_pos + false_neg) if true_pos + false_neg > 0 else 0\n",
        "    f1_score = 2 * ((precision * recall) / (precision + recall)) if precision + recall > 0 else 0\n",
        "\n",
        "    return accuracy, precision, recall, f1_score\n",
        "\n",
        "\n",
        "def baseline_50(ground_truth):\n",
        "    num_zeros = ground_truth.count(0)\n",
        "    num_ones = ground_truth.count(1)\n",
        "\n",
        "    half_zeros = num_zeros // 2\n",
        "    half_ones = num_ones // 2\n",
        "\n",
        "    flipped_zeros = 0\n",
        "    flipped_ones = 0\n",
        "\n",
        "    baseline_50 = []\n",
        "\n",
        "    for gt in ground_truth:\n",
        "        if gt == 0 and flipped_zeros < half_zeros:\n",
        "            baseline_50.append(1)\n",
        "            flipped_zeros += 1\n",
        "        elif gt == 1 and flipped_ones < half_ones:\n",
        "            baseline_50.append(0)\n",
        "            flipped_ones += 1\n",
        "        else:\n",
        "            baseline_50.append(gt)\n",
        "\n",
        "    return baseline_50\n",
        "\n",
        "\n",
        "def baseline_proportionate(ground_truth):      # predicts majority class 98.84% of the time\n",
        "    num_zeros = ground_truth.count(0)\n",
        "    num_ones = ground_truth.count(1)\n",
        "\n",
        "    proportion_zeros = int(num_zeros * 0.0116) # incorrect 1.16% of the time\n",
        "    proportion_ones = int(num_ones * 0.9884)   # incorrect 98.84% of the time\n",
        "\n",
        "    flipped_zeros = 0\n",
        "    flipped_ones = 0\n",
        "\n",
        "    baseline_proportionate = []\n",
        "\n",
        "    for gt in ground_truth:\n",
        "        if gt == 0 and flipped_zeros < proportion_zeros:\n",
        "            baseline_proportionate.append(1)\n",
        "            flipped_zeros += 1\n",
        "        elif gt == 1 and flipped_ones < proportion_ones:\n",
        "            baseline_proportionate.append(0)\n",
        "            flipped_ones += 1\n",
        "        else:\n",
        "            baseline_proportionate.append(gt)\n",
        "\n",
        "    return baseline_proportionate\n",
        "\n",
        "\n",
        "\n",
        "# Import label files\n",
        "with mgzip.open(f\"{tokenised_data_path}/BEAR_encoded_y_{dataset}.gz\", \"rb\") as f:\n",
        "    ground_truth = pickle.load(f)\n",
        "\n",
        "predictions = read_csv(f'{predictions_path}/predictions_{dataset}.csv')\n",
        "baseline_majority = [0 for _ in range(len(ground_truth))]                         # always predict negative (majority class)\n",
        "baseline_true_random = [random.choice([0, 1]) for _ in range(len(ground_truth))]  # randomly predict postive or negatice\n",
        "baseline_50_50_random = baseline_50(ground_truth)                                 # predict randomly such that it's correct 50% of the time on both classes\n",
        "baseline_proportionate_random = baseline_proportionate(ground_truth)              # predict randomly such that it's correct 98.837% of the time on both classes\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy, precision, recall, f1_score = calculate_metrics(baseline_proportionate_random, ground_truth)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.5f}')\n",
        "print(f'F1 Score: {f1_score:.5f}')\n",
        "print(f'Precision: {precision:.5f}')\n",
        "print(f'Recall: {recall:.5f}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
